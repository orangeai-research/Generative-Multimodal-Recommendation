#!/usr/bin/env python
# coding: utf-8
"""
å¯¹æ¯”å®éªŒè„šæœ¬ï¼šDiffMM vs RFMRec
è¿è¡Œä¸¤ä¸ªæ¨¡å‹å¹¶å¯¹æ¯”ç»“æœ
"""

import os
import sys
import subprocess
import json
import re
from datetime import datetime
import argparse

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))


def run_model(model_name, dataset='baby'):
    """è¿è¡ŒæŒ‡å®šæ¨¡å‹å¹¶æ•è·ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒ {model_name} æ¨¡å‹...")
    print(f"{'='*60}\n")

    # è®¾ç½®å·¥ä½œç›®å½•ä¸º srcï¼ˆå› ä¸º configurator ä» getcwd æ‰¾é…ç½®æ–‡ä»¶ï¼‰
    base_dir = os.path.dirname(os.path.abspath(__file__))
    src_dir = os.path.join(base_dir, 'src')

    # è¿è¡Œå‘½ä»¤
    cmd = [
        'python', 'main.py',
        '-m', model_name,
        '-d', dataset
    ]

    try:
        # è¿è¡Œå¹¶æ•è·è¾“å‡ºï¼ˆå·¥ä½œç›®å½•è®¾ä¸º srcï¼‰
        result = subprocess.run(
            cmd,
            cwd=src_dir,
            capture_output=True,
            text=True,
            timeout=7200  # 2å°æ—¶è¶…æ—¶
        )

        output = result.stdout + result.stderr

        # ä¿å­˜å®Œæ•´æ—¥å¿—
        log_dir = 'comparison_logs'
        os.makedirs(log_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(log_dir, f'{model_name}_{dataset}_{timestamp}.log')

        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(output)

        print(f"âœ… {model_name.upper()} è®­ç»ƒå®Œæˆï¼æ—¥å¿—ä¿å­˜åœ¨: {log_file}")

        # è§£æç»“æœ
        metrics = parse_results(output)
        return metrics, log_file

    except subprocess.TimeoutExpired:
        print(f"âŒ {model_name.upper()} è®­ç»ƒè¶…æ—¶ï¼ˆ2å°æ—¶ï¼‰")
        return None, None
    except Exception as e:
        print(f"âŒ {model_name.upper()} è®­ç»ƒå‡ºé”™: {str(e)}")
        return None, None


def parse_results(output):
    """ä»æ—¥å¿—è¾“å‡ºä¸­è§£ææŒ‡æ ‡"""
    metrics = {
        'valid': {},
        'test': {}
    }

    # æŸ¥æ‰¾æœ€ä½³ç»“æœéƒ¨åˆ†
    best_section = re.search(r'â–ˆâ–ˆâ–ˆâ–ˆCurrent BESTâ–ˆâ–ˆâ–ˆâ–ˆ:(.*?)(?=\n\n|$)', output, re.DOTALL)
    if not best_section:
        best_section = re.search(r'â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ BEST â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ(.*?)(?=\n\n|$)', output, re.DOTALL)

    if best_section:
        text = best_section.group(1)

        # è§£æ Valid ç»“æœ
        valid_match = re.search(r'Valid:\s*\{([^}]+)\}', text)
        if valid_match:
            valid_str = valid_match.group(1)
            for item in valid_str.split(','):
                item = item.strip()
                if ':' in item:
                    key, value = item.split(':', 1)
                    key = key.strip().strip("'\"")
                    value = value.strip()
                    try:
                        metrics['valid'][key] = float(value)
                    except ValueError:
                        pass

        # è§£æ Test ç»“æœ
        test_match = re.search(r'Test:\s*\{([^}]+)\}', text)
        if test_match:
            test_str = test_match.group(1)
            for item in test_str.split(','):
                item = item.strip()
                if ':' in item:
                    key, value = item.split(':', 1)
                    key = key.strip().strip("'\"")
                    value = value.strip()
                    try:
                        metrics['test'][key] = float(value)
                    except ValueError:
                        pass

    return metrics


def compare_results(results):
    """å¯¹æ¯”å¹¶å±•ç¤ºç»“æœ"""
    print(f"\n{'='*80}")
    print(f"{'æ¨¡å‹å¯¹æ¯”ç»“æœ':^80}")
    print(f"{'='*80}\n")

    models = list(results.keys())
    if len(models) < 2:
        print("âš ï¸  åªæœ‰ä¸€ä¸ªæ¨¡å‹çš„ç»“æœï¼Œæ— æ³•å¯¹æ¯”")
        return

    # è·å–æ‰€æœ‰æŒ‡æ ‡
    all_metrics = set()
    for model_results in results.values():
        if model_results:
            all_metrics.update(model_results['valid'].keys())
            all_metrics.update(model_results['test'].keys())

    # åˆ†åˆ«å±•ç¤º Validation å’Œ Test ç»“æœ
    for split in ['valid', 'test']:
        print(f"\n{'â”€'*80}")
        print(f"{split.upper()} SET ç»“æœå¯¹æ¯”")
        print(f"{'â”€'*80}")

        # è¡¨å¤´
        header = f"{'Metric':<20}"
        for model in models:
            header += f"{model.upper():>15}"
        header += f"{'Winner':>15}"
        print(header)
        print('â”€' * 80)

        # æ¯ä¸ªæŒ‡æ ‡
        metrics_list = sorted([m for m in all_metrics if '@' in m])  # åªæ˜¾ç¤ºå¸¦@çš„æŒ‡æ ‡

        for metric in metrics_list:
            row = f"{metric:<20}"
            values = []

            for model in models:
                if results[model] and metric in results[model][split]:
                    value = results[model][split][metric]
                    values.append((model, value))
                    row += f"{value:>15.4f}"
                else:
                    values.append((model, 0))
                    row += f"{'N/A':>15}"

            # æ‰¾å‡ºæœ€ä¼˜
            if values and any(v[1] > 0 for v in values):
                best_model = max(values, key=lambda x: x[1])[0]
                row += f"{best_model.upper():>15}"
            else:
                row += f"{'N/A':>15}"

            print(row)

    # ç»Ÿè®¡èƒœç‡
    print(f"\n{'â”€'*80}")
    print(f"èƒœç‡ç»Ÿè®¡ (åœ¨ TEST SET ä¸Š)")
    print(f"{'â”€'*80}")

    win_counts = {model: 0 for model in models}
    total_metrics = 0

    metrics_list = sorted([m for m in all_metrics if '@' in m])
    for metric in metrics_list:
        values = []
        for model in models:
            if results[model] and metric in results[model]['test']:
                value = results[model]['test'][metric]
                values.append((model, value))

        if values and any(v[1] > 0 for v in values):
            best_model = max(values, key=lambda x: x[1])[0]
            win_counts[best_model] += 1
            total_metrics += 1

    for model in models:
        win_rate = (win_counts[model] / total_metrics * 100) if total_metrics > 0 else 0
        print(f"{model.upper():<15} èƒœå‡º {win_counts[model]}/{total_metrics} æŒ‡æ ‡ ({win_rate:.1f}%)")

    # é‡ç‚¹æŒ‡æ ‡å¯¹æ¯”
    print(f"\n{'â”€'*80}")
    print(f"é‡ç‚¹æŒ‡æ ‡å¯¹æ¯” (Test Set)")
    print(f"{'â”€'*80}")

    key_metrics = ['recall@20', 'ndcg@20', 'precision@20', 'map@20']

    for metric in key_metrics:
        row = f"{metric.upper():<20}"
        values = []

        for model in models:
            if results[model] and metric in results[model]['test']:
                value = results[model]['test'][metric]
                values.append((model, value))
                row += f"{model.upper()}: {value:.4f}  "

        if len(values) == 2:
            improvement = ((values[1][1] - values[0][1]) / values[0][1] * 100) if values[0][1] > 0 else 0
            row += f"  (æ”¹è¿›: {improvement:+.2f}%)"

        print(row)

    print(f"\n{'='*80}\n")


def save_comparison_report(results, output_file='comparison_results.json'):
    """ä¿å­˜å¯¹æ¯”ç»“æœä¸º JSON"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    report = {
        'timestamp': timestamp,
        'models': results
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“Š å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {output_file}")


def main():
    parser = argparse.ArgumentParser(description='å¯¹æ¯” DiffMM å’Œ RFMRec æ¨¡å‹')
    parser.add_argument('--models', type=str, nargs='+', default=['DiffMM', 'RFMREC'],
                        help='è¦å¯¹æ¯”çš„æ¨¡å‹åˆ—è¡¨ (é»˜è®¤: DiffMM RFMREC)')
    parser.add_argument('--dataset', type=str, default='baby',
                        help='æ•°æ®é›†åç§° (é»˜è®¤: baby)')
    parser.add_argument('--output', type=str, default='comparison_results.json',
                        help='è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: comparison_results.json)')

    args = parser.parse_args()

    print(f"\n{'#'*80}")
    print(f"{'æ¨¡å‹å¯¹æ¯”å®éªŒ':^80}")
    print(f"{'#'*80}")
    print(f"\næ¨¡å‹: {', '.join([m for m in args.models])}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    results = {}

    # ä¾æ¬¡è¿è¡Œæ¯ä¸ªæ¨¡å‹
    for model in args.models:
        metrics, log_file = run_model(model, args.dataset)
        results[model] = metrics

        if metrics:
            print(f"\nğŸ“‹ {model} å…³é”®æŒ‡æ ‡é¢„è§ˆ:")
            if 'test' in metrics and 'recall@20' in metrics['test']:
                print(f"   Recall@20:  {metrics['test']['recall@20']:.4f}")
                print(f"   NDCG@20:    {metrics['test']['ndcg@20']:.4f}")

        print()

    # å¯¹æ¯”ç»“æœ
    compare_results(results)

    # ä¿å­˜æŠ¥å‘Š
    save_comparison_report(results, args.output)

    print(f"\nâœ… æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")


if __name__ == '__main__':
    main()
